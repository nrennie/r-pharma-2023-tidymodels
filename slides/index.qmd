---
title: "Introduction to machine learning with {tidymodels}"
subtitle: "R/Pharma 2023"
author: "Dr Nicola Rennie"
format:
  LUstyle-revealjs:
    footer: "[nrennie.github.io/r-pharma-2023-tidymodels](https://nrennie.github.io/r-pharma-2023-tidymodels)"
bibliography: references.bib
---

# Welcome!

## What to expect during this workshop

The workshop will run for *2 hours*.

* We'll take a 5 minute break halfway through.

* Combines slides, live coding examples, quiz questions, and exercises for you to participate in.

* Ask questions in the chat throughout!

## What to expect during this workshop

::: columns
::: {.column}

<br>

I hope you end up with more questions than answers after this workshop!

:::

::: {.column .center}

<br>

![](images/questions.gif){fig-align="center" fig-alt="Stranger Things questions gif" width=80%}

<small>Source: <a href="https://giphy.com/gifs/netflix-stranger-things-4-st4-cn7GcSZ1KWqO7CJw1B">giphy.com</a></small>

:::
:::


## Workshop resources

::: columns
::: {.column}

* GitHub: [github.com/nrennie/r-pharma-2023-tidymodels](https://github.com/nrennie/r-pharma-2023-tidymodels)

* Slides: [nrennie.github.io/r-pharma-2023-tidymodels](https://nrennie.github.io/r-pharma-2023-tidymodels)

:::
::: {.column}

![](images/qr-code.png){fig-align="center" fig-alt="QR code" width=70%}

:::

:::

## Data

We'll use two data sets in this workshop:

* Examples data: Heart Failure Clinical Records. [archive.ics.uci.edu/dataset/519/heart+failure+clinical+records](http://archive.ics.uci.edu/dataset/519/heart+failure+clinical+records)

```{r}
#| label: read-data-example
#| echo: true
#| eval: false
heart_failure <- readr::read_csv("data/heart_failure.csv") 
```

<br>

* Exercises data: Physical Exercise in Patients with Subacute Stroke (PHYS-STROKE): safety analyses of six-month follow-up of a randomized clinical trial. [doi.org/10.5281/zenodo.3899830](https://doi.org/10.5281/zenodo.3899830)

```{r}
#| label: read-data-exercises
#| echo: true
#| eval: false
exercises <- readr::read_csv("data/exercises.csv") 
```

## Quiz!

Have you done machine learning in R before?

* No

* Yes, with {tidymodels}

* Yes, with {caret}

* Yes, with something else

# Getting started with \{tidymodels\} {background-color="#D9DBDB"}

## What is {tidymodels}?

::: columns
::: {.column}

* A collection of R packages for statistical modelling and machine learning.

* Follows the {tidyverse} principles.

* `install.packages("tidymodels")`

:::

::: {.column}

![](images/tidymodels.png){fig-align="center" fig-alt="tidymodels R package hex sticker logo" width=50%}

:::
:::

## What is {tidymodels}?

::: columns

::: {.column}

There are some core {tidymodels} packages...

<br>

... and plenty of extensions!

:::

::: {.column .center}

![](images/tidymodels-pkgs.png){fig-align="center" fig-alt="tidymodels packages hex sticker logo" width=100%} 

<small>Source: <a href="https://rpubs.com/chenx/tidymodels_tutorial">rpubs.com/chenx/tidymodels_tutorial</a></small>

:::

:::

## What is machine learning?

::: columns
::: {.column}

![](images/ML-01.png){fig-align="center" fig-alt="Machine learning diagram" width=80%} 

:::

::: {.column}

* Learning from data

* Mostly used to make predictions or classifications.

:::
:::

## What is machine learning?

::: columns
::: {.column}

![](images/ML-02.png){fig-align="center" fig-alt="Machine learning diagram" width=80%} 

:::

::: {.column}

* Learning from data

* Mostly used to make predictions or classifications.

:::
:::

## Types of machine learning

::: columns

::: {.column}

**Supervised learning**: requires labelled input data

* Classification

* Regression-based models

* ...

:::

::: {.column}

**Unsupervised learning**: does not require labelled input data

* Clustering

* Association rules

* ...

:::

:::

Other types of machine learning include *semi-supervised learning* and *reinforcement learning*.

# Before we start fitting models... {background-color="#D9DBDB"}

## Training and testing data

![](images/TrainTest.png){fig-align="center" fig-alt="Training and testing diagram" width=70%} 

## Hyperparameter tuning

::: columns

::: {.column width="30%"}

We can't always learn every parameter from the data.

:::

::: {.column .fragment width="70%"}

![](images/kfold.jpg){fig-align="center" fig-alt="kfold validation" width=80%} 

<small>Source: <a href="https://www.researchgate.net/publication/332370436_Introduction_to_Support_Vector_Machines_and_Kernel_Methods">Introduction to Support Vector Machines and Kernel Methods. Ashfaque and Iqbal. 2019.</a></small>

:::

:::

## Workflows and recipes

::: columns

::: {.column width="30%"}

![](images/recipes.png){fig-align="center" fig-alt="recipes package hex sticker" width=80%} 

:::

::: {.column width="70%"}

**Recipe**

A series of preprocessing steps performed on data before you fit a model.

<br>

**Workflow**

An object that can combine your pre-processing, modelling, and post-processing steps. E.g. combine a `recipe` with a model.


:::

:::

## Pre-processing in {tidymodels}

### Live demo!

. . . 

<br><br>

See `examples/example_01.R` for full code.

## Exercise 1

Open `exercises/exercise_01.R` for prompts.

* Load the {tidyverse} and {tidymodels} packages

* Read in the `exercises.csv` data

* View and explore the data

* Perform the initial split (choose your own proportion!)

* Create some cross-validation folds

* Build a recipe and workflow

```{r}
#| label: ex-1-timer
countdown::countdown(minutes = 10,
                    color_border = "#b20e10",
                    color_text = "#b20e10",
                    color_running_text = "white",
                    color_running_background = "#b20e10",
                    color_finished_text = "#b20e10",
                    color_finished_background = "white",
                    top = 0,
                    margin = "1.2em",
                    font_size = "2em")
```

. . . 

See `exercise_solutions/exercise_solutions_01.R` for full code.

# LASSO regression {background-color="#D9DBDB"}

## Linear and logistic regression models

Let's go back a little bit first... 

. . . 

::: columns

::: {.column}
**Linear regression**
```{r}
#| label: lin-reg
#| eval: false
#| echo: true
lm(y ~ x, data = model_data)
```

![](images/Lasso_01.png){fig-align="center" fig-alt="Linear regression plot" width=70%}
:::

::: {.column .fragment}
**Logistic regression**

```{r}
#| label: log-reg
#| eval: false
#| echo: true
glm(y ~ x, family = "binomial", data = model_data)
```
![](images/Lasso_02.png){fig-align="center" fig-alt="Linear regression plot" width=70%}
:::

:::

## Quiz!

How do you choose which explanatory variables to include?

* Using background knowledge

* p-values and correlations between variables

* Stepwise procedures (forward/backward/bi-directional)

* Something else

## LASSO regression

::: columns
::: {.column}

**Standard regression**: minimise distance between predicted and observed values

<br>

**Least Absolute Shrinkage and Selection Operator (LASSO)**: minimise (distance between predicted and observed values + $\lambda$ $\times$ sum of coefficients)

<br>

See also: ridge regression

:::
::: {.column .fragment}

![](images/lambda.png){fig-align="center" fig-alt="Lambda plot for lasso regression" width=100%}

<small>Source: <a href="https://www.cvxpy.org/examples/machine_learning/lasso_regression.html">cvxpy.org/examples/machine_learning/lasso_regression.html</a></small>

:::
:::

## Hyperparameters for LASSO regression

**$\lambda$ (penalty)** takes a value between 0 and $\infty$.

* Higher value: more coefficients are pushed towards zero

* Lower value: closer to standard regression models. ($\lambda = 0$ ~ standard regression model)

## Model evaluation

::: columns
::: {.column}

**(Binary) Classification Metrics**

* Accuracy: proportion of the data that are predicted correctly.

* ROC AUC: area under the ROC (receiver operating characteristic) curve.

* Kappa: similar to accuracy but normalised by the accuracy expected by chance alone.

:::
::: {.column .fragment .center}

![](images/roc.png){fig-align="center" fig-alt="ROC curve" width=100%}

<small>Source: <a href="https://commons.wikimedia.org/wiki/File:Roc-draft-xkcd-style.svg#filelinks">Martin Thoma (Wikipedia)</a></small>
:::
:::

See [yardstick.tidymodels.org/articles/metric-types.html](https://yardstick.tidymodels.org/articles/metric-types.html).

## LASSO logistic regression in {tidymodels}

### Live demo!

. . . 

<br><br>

See `examples/example_02.R` for full code.


## Exercise 2

Open `exercises/exercise_02.R` for prompts. You can also use `examples/example_02.R` as a starting point.

* Specify the model using `logistic_reg()`.

* Tune the hyperparameter.

* Choose the best value and fit the final model.

* Evaluate the model performance.

```{r}
#| label: ex-2-timer
countdown::countdown(minutes = 10,
                    color_border = "#b20e10",
                    color_text = "#b20e10",
                    color_running_text = "white",
                    color_running_background = "#b20e10",
                    color_finished_text = "#b20e10",
                    color_finished_background = "white",
                    top = 0,
                    margin = "1.2em",
                    font_size = "2em")
```

. . . 

See `exercise_solutions/exercise_solutions_02.R` for full code.

## Quiz!

What was the ROC AUC of your LASSO model?

* Less than 70%

* 70-80%

* 80-90%

* 90-100%

# Random Forests {background-color="#D9DBDB"}

## Decision trees

::: columns
::: {.column}

A tree-like model of decisions and their possible consequences.

:::
::: {.column}

![](images/rf_01.png){fig-align="center" fig-alt="Decision tree about walking to work and the weather" width=100%}

:::
:::

## What are Random Forests?

::: columns
::: {.column}

* An ensemble method

* Combines many decision trees.

* Can be used for classification or regression problems.

* For classification tasks, the output of the random forest is the class selected by most trees.

:::
::: {.column}

![](images/rf.png){fig-align="center" fig-alt="Random Forest diagram" width=100%}

<small>Source: <a href="https://en.m.wikipedia.org/wiki/File:Random_forest_explain.png">Tse Ki Chun (Wikimedia)</a></small>

:::
:::

## Hyperparameters for random forests

**trees**: number of trees in the ensemble.

<br>

**mtry**: number of predictors that will be randomly sampled at each split when creating the tree models.

<br>

**min_n**: minimum number of data points in a node that are required for the node to be split further.

## Random Forests in {tidymodels}

### Live demo!

. . . 

<br><br>

See `examples/example_03.R` for full code.

## Exercise 3

Open `exercises/exercise_03.R` for prompts. You can also use `examples/example_03.R` as a starting point.

* Specify a random forest model using `rand_forest()`

* Tune the hyperparameters using the cross-validation folds.

* Fit the final model and evaluate it.

```{r}
#| label: ex-3-timer
countdown::countdown(minutes = 10,
                    color_border = "#b20e10",
                    color_text = "#b20e10",
                    color_running_text = "white",
                    color_running_background = "#b20e10",
                    color_finished_text = "#b20e10",
                    color_finished_background = "white",
                    top = 0,
                    margin = "1.2em",
                    font_size = "2em")
```

. . . 

See `exercise_solutions/exercise_solutions_03.R` for full code. 

# Support Vector Machines (SVM) {background-color="#D9DBDB"}

## What are Support Vector Machines?

::: columns
::: {.column}

Support Vector Machines (SVMs) draw a decision boundary that best separates two groups.

:::
::: {.column}

![](images/svm_01.png){fig-align="center" fig-alt="Scatter plot with two groups" width=100%}
:::
:::

## What are Support Vector Machines?

::: columns
::: {.column}

Support Vector Machines (SVMs) draw a decision boundary that best separates two groups.

:::
::: {.column}

![](images/svm_02.png){fig-align="center" fig-alt="Scatter plot with two groups with dividing line" width=100%}
:::
:::

## Types of SVM

There are different types of *kernel* functions, including:

::: columns
::: {.column width="33%" .center}
Linear

`svm_linear()`

![](images/svm_03.png){fig-align="center" fig-alt="linear kernel plot" width=100%}
:::

::: {.column width="33%" .center}
Polynomial

`svm_poly()`

![](images/svm_04.png){fig-align="center" fig-alt="polynomial kernel plot" width=100%}
:::

::: {.column width="33%" .center}
Radial Basis Functions

`svm_rbf()`

![](images/svm_05.png){fig-align="center" fig-alt="RBF kernel plot" width=100%}
:::

:::

## Hyperparameters in SVMs

**Cost**

* Higher value: emphasises fitting the data

* Lower value: prioritises avoiding overfitting

**Gamma** (shape and smoothness of decision boundary)

* Higher value: more flexible boundaries

* Lower value: simpler boundaries

There may be other hyperparameters, depending on the choice of kernel.

## Support Vector Machines in {tidymodels}

### Live demo!

. . . 

<br><br>

See `examples/example_04.R` for full code.

## Exercise 4

Open `exercises/exercise_04.R` for prompts. You can also use `examples/example_04.R` as a starting point.

* Specify a support vector machine using `svm_rbf()` (or one of the other `svm_*` functions if you're feeling confident!)

* Tune the `cost()` hyperparameter using the cross-validation folds.

* Fit the final model and evaluate it.

* Look at some other evaluation metrics.

```{r}
#| label: ex-4-timer
countdown::countdown(minutes = 10,
                    color_border = "#b20e10",
                    color_text = "#b20e10",
                    color_running_text = "white",
                    color_running_background = "#b20e10",
                    color_finished_text = "#b20e10",
                    color_finished_background = "white",
                    top = 0,
                    margin = "1.2em",
                    font_size = "2em")
```

. . . 

See `exercise_solutions/exercise_solutions_04.R` for full code.

## Quiz

Which of the three models performed best on the `exercises` data?

* LASSO

* Random Forest

* SVM

# Additional Information {background-color="#D9DBDB"}

## Learning more about machine learning

There are many machine learning topics and {tidymodels} functions we haven't covered today. As well as learning about the technical details of models and how to write code, it's important to learn about:

* Ethics

* Bias and discrimination

* Explainability and validation

* ...

## Additional resources

::: columns

::: {.column}

<br>

* [{tidymodels} documentation](https://www.tidymodels.org/)

* [Tidy Modeling with R](https://www.tmwr.org/)

* [Blog by Julia Silge](https://juliasilge.com/blog/)

:::

::: {.column}

![](images/tmwr.png){fig-align="center" fig-alt="Tidy modelling with R cover" width=60%}

:::

:::

## Workshop resources

* GitHub: [github.com/nrennie/r-pharma-2023-tidymodels](https://github.com/nrennie/r-pharma-2023-tidymodels)

* Slides: [nrennie.github.io/r-pharma-2023-tidymodels](https://nrennie.github.io/r-pharma-2023-tidymodels)


## 

::: columns
::: {.column}

<br>

{{< fa brands linkedin >}} [nicola-rennie](https://www.linkedin.com/in/nicola-rennie/)

{{< fa brands mastodon >}} [\@fosstodon.org/\@nrennie](https://fosstodon.org/deck/@nrennie)

{{< fa brands github >}} [nrennie](https://github.com/nrennie)

{{< fa globe >}} [nrennie.rbind.io](https://nrennie.rbind.io/)

{{< fa briefcase >}} [chicas.lancaster-university.uk](https://chicas.lancaster-university.uk/)

:::
::: {.column}

![](images/qr-code.png){fig-align="center" fig-alt="QR code" width=70%}

:::
:::
